\chapter{Stand van zaken}
\label{ch:stand-van-zaken}

% Tip: Begin elk hoofdstuk met een paragraaf inleiding die beschrijft hoe
% dit hoofdstuk past binnen het geheel van de bachelorproef. Geef in het
% bijzonder aan wat de link is met het vorige en volgende hoofdstuk.

% Pas na deze inleidende paragraaf komt de eerste sectiehoofding.

Dit hoofdstuk bevat een literatuurstudie omtrent data warehousing. Na het lezen van dit hoofdstuk zullen begrippen zoals dimensioneel modelleren, Data Vault 2.0 en data warehousing jou niet meer onbekend zijn en waarom er nood is aan data warehousing. Je zal in staat zijn om een eerste Data Vault model en een dimensioneel model te kunnen schetsen.

%\textcite{Knuth1998}
\section{Inleiding data warehousing}

\epigraph{Torture the data, and it will confess to anything. }{\textit{Ronald Coase \\ Winnaar Nobelprijs in Economie (1991)}}

Veel moderne, digitale bedrijven genereren tegenwoordig enorme volumes data. Deze data kan afkomstig zijn uit verschillende bronnen: ERP/CRM-systeem, flat-files (bijvoorbeeld Excel-sheets), big data, ... Bestuursleden gebruiken data om beslissingen te nemen die de onderneming toelaat om te (blijven) groeien of om bepaalde problemen op te sporen. Stel dat een onderneming meer kosten heeft dan opbrengsten, dan kunnen we op basis van alle gegevens die het bedrijf bezit een analyse maken. Zijn er overbodige kosten? Worden onze producten/diensten aan een te lage prijs verkocht? Dit zijn maar enkele vragen die kunnen opgelost worden wanneer het bestuur de correcte rapporteringen ontvangt. Ook wordt een data warehouse gebruikt om gegevens aan te leveren om budgetten op te stellen voor de komende jaren en om van forecasting gebruik te maken. Zo kan er bijvoorbeeld een beslissing gemaakt worden welk budget er moet toegekend worden aan de verschillende afdelingen. 

Om alles nog even samen te vatten: een data warehouse levert data aan een rapporteringsomgeving. Op basis van deze data worden beslissingen ondersteund.

\subsection{Soorten data}
Er kan een onderscheid gemaakt worden tussen verschillende soorten data. Voornamelijk kunnen we informatie opdelen in drie categorieën: gestructureerde, semi-gestructureerde en ongestructureerde data. Volgens  ~\textcite{Langseth2005}, bestaat 95\% van de globale informatie uit ongestructureerde data. 

\subsubsection{Gestructureerde data}
Data afkomstig uit een relationele databank (RDBMS) is meestal gestructureerd. Deze data is meestal ingedeeld in categorieën, denk bijvoorbeeld maar aan postcode, naam, klantennummer, ... Hieruit volgt dat deze data heel gemakkelijk te doorzoeken is. 

\subsubsection{Semi-gestructureerde data}
Data afkomstig uit IoT-apparaten of XML-bestanden zijn meestal semi-gestructureerd. In deze data is een bepaalde structuur te herkennen, maar deze data zit niet in een bepaalde tabel-structuur.

\subsubsection{Ongestructureerde data}
Deze informatie kan niet gemakkelijk worden opgeslagen in relationele databanken (er kan hiervoor geen primair datatype gebruikt worden, maar bijvoorbeeld wel als een BLOB-type). Denk maar aan Excel-sheets, emails, muziek, ... Deze data bevat vaak ook heel nuttige informatie die organisaties graag willen gebruiken. Denk bijvoorbeeld maar aan emails: hoe gelukkig zijn klanten over een bepaald product? Hoeveel mails worden er maandelijks ontvangen met klachten?

\subsection{Wat is een data warehouse?}
De definitie van een data warehouse luidt als volgt: \textit{'een subject-georiënteerde, geïntegreerde, tijd-variante, niet-vluchtige collectie van gegevens die in eerste instantie gebruikt wordt bij organisaties om beslissingen te nemen'} (Bill Inmon).

\paragraph{Subject-georiënteerd}
Dit begrip slaat op het feit dat een data warehouse gebouwd is met als doel het analyseren van data, niet om transacties op toe te passen. Dit wordt uitgebreid besproken in subsectie \ref{sec:oltp-vs-olap}.


\paragraph{Geïntegreerd}
Dit betekent dat de data warehouse een 'centrale' databank is die gegevens bevat vanuit verschillende bronsystemen (bijvoorbeeld gegevens uit het klantenbestand en gegevens uit het verkoopsysteem). 

\paragraph{Tijd-variant}
Alle data van het verleden, moet terug te vinden zijn in de data warehouse. Dit betekent dat data uit het verleden (bijvoorbeeld een vorige factuur van een klant) moet beschikbaar zijn, ook al is de data in het transactioneel systeem aangepast. Dit wordt toegepast op transactionele data, minder op master data.

\paragraph{Niet-vluchtig}
De data die in het systeem zit, moet onveranderlijk zijn, ook al is de data foutief. Om de foutieve data toch aan te passen, zal er een nieuwe rij moeten toegevoegd worden die de juiste data bevat, die een hogere versie bevat dan de vorige rij. 

\paragraph{Conclusie}
We kunnen dus uit de definitie van een data warehouse afleiden dat het een grote databron is die alle (gestructureerde semi-gestructureerde en ongestructureerde) gegevens bevat die een organisatie bezit vanaf het moment dat de data warehouse geïmplementeerd werd tot het heden. Op deze databron worden dan analyses gemaakt.

\subsection{Waarom is er nood aan een data warehouse?}
Een organisatie heeft tegenwoordig heel wat data ter beschikking. Vaak is deze data gefragmenteerd over verschillende systemen. Wanneer men een analyse wil maken op basis van de verspreide data, zal dat niet evident zijn. 

Om deze reden wordt een data warehouse ontworpen. Hierin worden gegevens, verspreid over meerdere bronnen, in één centrale plaats verzameld. Zo kunnen rapporteringen makkelijk en flexibel opgebouwd worden. 

Een andere reden voor het opbouwen van een data warehouse is dat je de historiek van alle data kan bijhouden. Wanneer er bijvoorbeeld gegevens aangepast zijn in het transactionele systeem, dan zijn de oude gegevens vaak moeilijk te achterhalen (omdat deze dat vaak overschreven wordt). Door verschillende versies bij te houden van entiteiten, kan je oudere data makkelijk opzoeken.

Wanneer men rapporteringen wil opvragen aan het transactionele systeem, vergt dit ook extra belasting van de server. Dit komt doordat dat datamodel opgebouwd werd niet geoptimaliseerd is om zware SELECT-queries af te handelen. Dit zou niet alleen de server (van het transactionele systeem) meer belasten, bovendien zal dit ook zorgen voor een tragere rapportering. Indien de server te veel rapporteringen zou moeten opvragen, zou dit kunnen leiden tot een overbelasting waarbij transacties niet meer mogelijk zouden kunnen zijn. 


\subsection{Wat is het doel van een data warehouse?}
Het belangrijkste doel van een data warehouse is om een \textbf{correcte} rapportering te leveren. Dit zorgt ervoor dat het beslissingsnemingsproces ondersteunt wordt die genomen wordt door het management. 

\paragraph{Data kwaliteit}
Zoals eerder aangekaart, is het belangrijk dat rapporten de juiste gegevens bevat. Hieruit volgt dat data kwaliteit een heel belangrijk aspect is. Vaak zijn er verschillende oorzaken waarom de data kwaliteit niet voldoet:

\begin{itemize}
	\item Inconsistente data tussen verschillende systemen
	\item Incorrecte gegevens
	\item Onvoldoende validatie bij het invoeren van gegevens
	\item Onjuiste gegevensbewerkingen
	\item \ldots
\end{itemize}  
~\autocite{Helfert2002}

Gegevens die in een data warehouse geladen worden, ondergaan een proces (zie paragraaf \ref{sec:etl}). In dit proces wordt de data gemanipuleerd zodat de data kwaliteit verhoogd wordt.

\paragraph{Performantie}
We kunnen een onderscheid maken tussen 3 verschillende soorten beslissingen: operationele (dagelijks), tactische (jaarlijks) en strategische (lange termijn) beslissingen. Wanneer we operationele rapporten nodig hebben, verwachten we dan ook dat deze onmiddelijk kunnen opgeleverd worden. Het datamodel van een data warehouse wordt geoptimaliseerd voor het ophalen van data in plaats van het te kunnen stockeren. De data wordt 's nachts ingeladen zodat werknemers geen performantieproblemen hieromtrend ondervinden. 

\subparagraph{De toekomst}
Door de komst van in-memory databanken merken we op dat een deel van de (operationele) rapportering opnieuw verhuist naar de transactionele databanken. Dit heeft enerzijds te maken met de snelheid van de databanken en anderzijds met het feit dat queries om operationele rapporten op te vragen gebruikelijk niet zo belastend zijn. Zo kan er gewerkt worden met \textbf{live data} (doordat deze niet 's nachts moet ingeladen worden in de data warehouse). De voorwaarde hiervoor is dat alle benodigde data beschikbaar is binnen dat geïntegreerd systeem. Een voorbeeld van een in-memory databank is HANA, een technologie ontwikkelt door SAP.

\paragraph{Automatisering}
Doordat alle rapporteringsnoden geautomatiseerd kunnen worden, heeft dit natuurlijk als voordeel dat personeelsleden deze niet meer manueel hoeven te maken/berekenen. Zo kunnen ze hun tijd spenderen aan andere prioriteiten. Deze data kan dan worden voorgesteld in een overzichtelijke omgeving (zie sectie \ref{sec:omgeving}). Bovendien zal er ook een kleinere kans zijn op fouten.

\subsection{Wat is OLTP en wat zijn de verschillen met OLAP?}
\label{sec:oltp-vs-olap}
On-line transactional processing (OLTP) systemen zijn voornamelijk klantgericht. Het datamodel is opgebouwd rond het efficiënt verwerken van transacties. On-line analytical processing (OLAP) systemen zijn martkgericht. De data in een OLAP-systemen worden gebruikt om analyses op uit te voeren ~\autocite{Satyanarayana2010}.


\paragraph{Inhoudelijk}
Bij OLAP systemen worden metadata opgeslagen bij de entiteiten. Voorbeelden hiervan zijn: tijdstip van inladen, van welke bron de data komt, ... Het grote voordeel hierbij is dat wanneer een fout gebeurt, er gemakkelijker kan achterhaald worden waar het fout liep. Ook wordt de historische data ook bewaard, in tegenstelling tot OLTP. Bij OLTP wordt de te wijzigen data overschreven. Het gevolg hiervan is dat de volume data bij OLAP doorgaans groter zal zijn.

\paragraph{Toegankelijkheid}
Wanneer men data wil verkrijgen/wijzigen in een OLTP systeem, moet er rekening gehouden met een aantal aspecten. Een transactie in een OLTP omgeving moet voldoen aan enkele eisen:
\begin{itemize}
	\item \textbf{Atomic:} Wanneer een transactie afgebroken is, mag er niets gewijzigd zijn in de databank.
	\item \textbf{Consistent:} Als een deel van de transactie faalt, zullen alle doorgevoerde wijzigingen in die transactie ongedaan gemaakt worden en zal de databank terugkeren naar een consistente staat.
	\item \textbf{Isolated:} Transacties worden geïsoleerd, transacties mogen in geen enkel geval elkaar beïnvloeden.
	\item \textbf{Durable:} Wanneer een transactie is doorgevoerd, kan deze niet meer ongedaan gemaakt worden.
\end{itemize}  

Bij een OLAP-systeem worden geen transacties doorgevoerd, enkel leesopdrachten. Dat vermindert de complexiteit en verhoogt de snelheid van de queries ~\autocite{Satyanarayana2010}. 

\subsection{Wat zijn de benodigdheden voor een data warehouse?}
Voor er kan begonnen worden met het opbouwen van een data warehouse, zijn er enkele benodigdheden. Zo zal er een keuze moeten gemaakt worden voor een bepaalde methodologie en een architectuur. Ook zal er een fysieke opslagplaats nodig zijn. Hiervoor kan gebruik gemaakt worden van Cloud oplossingen of een on-premise server. Maar in dit hoofdstuk bespreken we welke software-aspecten er nodig zijn bij het opbouwen van de data warehouse.

\subsubsection{RDBMS}
Voor het opmaken en beheren van de data warehouse zal er een RDBMS moeten gekozen worden. Hiervoor zijn heel wat mogelijkheden beschikbaar op de markt. Bijvoorbeeld: 

\begin{itemize}
	\item Oracle DB
	\item Microsoft SQL Server
	\item IBM DB2
	\item Microsoft Office Access
	\item \ldots
\end{itemize} 

Klassieke databanken slaan hun gegevens op op harde schijven en/of SSD's. Maar sinds kort zien we de komst van een nieuwe technologie, genaamd een in-memory databank. Hierbij worden de gegevens initieël opgeslagen in het RAM-geheugen. Dit zorgt voor een veel snellere lees- en schrijftijd. Het nadeel van deze nieuwe technologie is het prijskaartje.

\subsubsection{Data integratie software}
\label{sec:etl}
Het data inladen is een proces dat verantwoordelijkheid draagt voor de data integratie software, al is dat niet zijn enige verantwoordelijkheid. De software is verantwoordelijk voor het gehele ETL-proces (zie visualisatie \ref{fig:etl}):

\begin{itemize}
	\item Extraction: Ophalen van de data vanuit de bron
	\item Transformation: Transformaties en manipulaties uitvoeren op die data
	\item Load: De data wegschrijven naar de nieuwe bron
\end{itemize} 

\begin{figure}[h]
	\centering
	\includegraphics[scale=1]{../images/ETL.PNG}
	\caption{Het ETL-proces}
	\label{fig:etl}
\end{figure}

\pagebreak

\section{Dimensioneel modelleren via Kimball}
Wie consultant Business Intelligence is, heeft ongetwijfeld al gehoord van dimensioneel modelleren. Over de jaren heen is het als het ware een standaard geworden wanneer men een data warehouse wil ontwerpen. Dimensioneel modelleren heeft als voordeel dat het model niet complex is, dus gemakkelijk te begrijpen, zelf voor niet IT-opgeleide personen. Ook is er vaak een snel resultaat beschikbaar. In deze sectie zal het dimensioneel modelleren aan de hand van Kimball dieper bekeken worden. 


\subsection{Architectuur}
Bij het dimensioneel modelleren wordt het proces opgedeeld in 2 soorten lagen: de staging area en de data warehouselaag. In de data warehouselaag worden de data marts opgebouwd. Rapporteringsomgevingen connecteren met de data marts om hun data op te halen.

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.8]{../images/dimmodel.PNG}
	\caption{Architectuur van een data warehouse op basis van het dimensioneel modelleren.}
	\label{fig:dimmodel}
\end{figure}

\subsubsection{Staging area}
\label{sec:stagingarea}
In deze laag wordt het ETL-proces toegepast. Eerst en vooral wordt de data vanuit één of meerdere bronnen in de data warehouse geladen. Daarna wordt de data bewerkt en gemanipuleerd. Bijvoorbeeld records met bepaalde lege waarden weglaten. De data kwaliteit (zoals eerder aangekaart) is zeer belangrijk in een data warehouse. Het doel is om de toegekomen data consistent te maken en ervoor te zorgen dat de integriteit gewaarborgd blijft ~\autocite{Kimball2013}

\pagebreak

\subsubsection{Data warehouselaag}
In deze laag worden OLAP-cubes of relationele ster schema's gemaakt op basis van de staging area. Deze laag kan eigenlijk als de presentatielaag beschouwd worden. Deze laag moet gedetailleerde data bevatten. Een data mart moet gebaseerd zijn rond een business unit ~\autocite{Kimball2013}.

\subsection{Componenten}
Een sterschema of OLAP-cube bestaat uit dimensies en facts. Wat deze precies zijn, wordt hieronder uitgelegd. 

\subsubsection{Dimension tabel}
Voor elke dimensie wordt een primary key aangemaakt (of afkomstig uit het systeem als business sleutel). Deze sleutel wordt gebruikt in een fact tabel als foreign key, zodat er een relatie kan worden gelegd tussen beide attributen.

Naast de primary key wordt in deze tabel ook de beschrijvende data bewaard voor een bepaalde rij. Deze data kan gebruikt worden om in de rapporteringsomgeving de verschillende assen te kiezen. Een typisch attribuut is bijvoorbeeld 'naam' of 'woonplaats' voor de dimensie 'Customer'.

\subsubsection{Fact tabel}
In een fact tabel worden alle meetbare cijfers bijgehouden. Meetbare cijfers betekent dat bewerkingen moeten mogelijk zijn op die data. Een bankrekeningnummer is een getal, maar hier kunnen geen bewerkingen met uitgevoerd worden (bijvoorbeeld gemiddelde bankrekeningnummer geven). Facturatiebedrag is een goed voorbeeld. Hierop kunnen enkele bewerkingen worden uitgevoerd, bijvoorbeeld: gemiddelde, minimum, maximum, totaal, .... Deze gegevens worden in vaktermen als \textbf{measures} aangeduid. 

De fact tabel bevat niet alleen measures, maar ook de foreign keys van de dimensies waarmee het verbonden is. Zo kan je bruikbare informatie toevoegen aan je measure in de rapporteringsomgeving.

\subsubsection{Ster schema}
In een ster schema worden dimensies en fact tabellen verbonden door enerzijds de primaire sleutels in de dimensies, en anderzijds bij de vreemde sleutels in de fact tabel. Wanneer meerdere tabellen verbonden worden met elkaar zien we een centraal punt in het model, dat de fact tabel is (zie figuur \ref{fig:ster}). 

\begin{figure}[h]
	\includegraphics[scale=0.8]{../images/ster.PNG}
	\caption{Ster schema voorgesteld door \textcite{Kimball2013}.}
	\label{fig:ster}
\end{figure}

\subsubsection{Verschil tussen een sterschema en een OLAP-cube}
Het verschil tussen beide zit niet in het ontwerp, maar puur in het 'fysieke' gedeelte.
OLAP cubes zijn geoptimaliseerd voor een drill down of een drill up te doen in de gegevensset.
Drill down betekent dat de gegevens op een dieper detailniveau zullen bekeken worden, bijvoorbeeld vertrekkend uit een productcategorie niveau, ga je naar een productniveau. OLAP cubes zorgen ervoor dat er meer anlytische functies beschikbaar zijn in vergelijking met SQL. Maar als nadeel heeft de OLAP cubes dat het niet zo performant is als een ster schema  (zie figuur \ref{fig:stervsolap}).  \autocite{Kimball2013}. 

\begin{figure}[h]
	\includegraphics[scale=0.8]{../images/starvsolap.PNG}
	\caption{Sterschema (links) en OLAP cube (rechts) voorgesteld door \textcite{Kimball2013}.}
	\label{fig:stervsolap}
\end{figure}

\section{Modelleren via Data Vault 2.0}
Data Vault 2.0 is een modelleertechniek die ontworpen is door Daniel Linstedt. Het model zorgt ervoor dat dimensies gemakkelijk uitgebreid kunnen worden en dat databronnen toevoegen vlot moet gaan. Linstedt is van mening dat business requirements vaak veranderen, dus moet het model waarin de data warehouse ontworpen is ook flexibel zijn. 
~\autocite{Linstedt2016}
\subsection{Architectuur}
Data vault maakt gebruik van een 3-lagen model. Dit heeft als voordeel dat processen duidelijk kunnen onderscheiden worden per laag en alles overzichtelijk blijft. Deze architectuur ondersteunt om data op halen via een batch, maar ook om live data op te halen. NoSQL kan ook gebruikt worden om de data warehouse te ontwerpen. Wanneer de data live opgehaald wordt, valt de staging area weg en wordt de data onmiddelijk in de raw data vault geladen.  ~\autocite{Linstedt2016}


\begin{figure}[h]
	\includegraphics[scale=0.63]{../images/DVArchitectuur.png}
	\caption{Data Vault architectuur voorgesteld door \textcite{Stroobants2018}.}
	\label{fig:dvarch}
\end{figure}

\subsubsection{Staging area}
In deze laag wordt alle data ingeladen (of een virtuele tabel gebruikt) van een bepaalde bron via een batch. Hier wordt alle data onbewerkt ingeladen. Deze data bevat dan ook nog geen historische metadata. De tabel worden dus gedupliceerd van de bron(nen). Deze laag is niet persistent.

\subsubsection{Raw data vault}
De data wordt overgeladen van de staging area naar de raw data vault via het ETL-proces (\ref{sec:etl}). Deze laag is persistent, logisch ook wanneer we verschillende versies en de historiek behouden van onze entiteiten. Vanaf deze laag beginnen we te modelleren in Data Vault. De data wordt in deze laag gemanipuleerd (ofwel getransformeerd (ETL)). Ook wordt er metadata toegevoegd aan de records zodat er audits kunnen plaatsvinden. 

\subsubsection{Business vault}
Dit is een optionele laag. Deze laag wordt enkel en alleen toegevoegd wanneer er 'Business regels' moet toegepast worden in het model. De business vault wordt in principe niet opgeslagen in een aparte laag, maar vaak wordt deze opgeslagen als een uitbreiding van de raw data vault. Een voorbeeld van een 'Business rule' is dat je bijvoorbeeld geen producten wil promoten wanneer er minder dan 10 in voorraad zijn.

\subsubsection{Information marts}
Vertrekkend uit de business vault (of raw data vault) zullen er information marts moeten aangemaakt worden, ook wel bekend als data marts . ~\textcite{Linstedt2016} spreek liever over 'information' mart omdat het doel van een enterprise data warehouse duidelijk is: informatie aanbieden. 
De information marts bestaan uit sterschema's. Hierop connecteren de eindgebruikers om hun informatie te verkrijgen. 

\subsection{Componenten}
Een data vault model bestaat uit 3 soorten componenten: hubs, links en sattelites. Elk component heeft zijn functie en zijn doel.

\subsubsection{Hubs}
\textcite{Linstedt2016} beschrijft hubs als pilaren voor het Data Vault model. Een hub bestaat altijd uit minimaal 4 attributen:

\begin{itemize}
	\item \textbf{Hashkey (PK):} Als primary key van de entiteit wordt een gehashte identifier van de entiteit gebruikt.
	\item \textbf{LoadDate:} Datum/tijdstip wanneer de record geëxtraheerd werd uit de bron.
	\item \textbf{Record source:} Van welke databron is de record afkomstig?
	\item \textbf{Business key(s):} De business key(s) van de bijhorende entiteit
\end{itemize} 

 De business key is een unieke sleutel, die vaak een betekenis heeft naar de business. Voorbeelden zijn: ISBN-nummers, Klantnummer, Chassisnummer, ...
 
 Hubs zijn vooral heel handig wanneer er meerdere databronnen zijn, zo kan je meerdere bronnen aan een hub hangen. In een hubs zit nooit andere data behalve een hash key, metadata en business keys.

\subsubsection{Links}
Wanneer we 2 hub-entiteiten willen verbinden, zullen we ze niet rechtstreeks met elkaar verbinden. Twee hub-entiteiten worden namelijk verbonden door middel van een link. Andere verantwoordelijkheden voor een link zijn hierarchieën, redefinities of business termen. De bedoeling is om een zo laag mogelijke granulariteit te creeëren. Links zorgen ervoor dat het data vault model heel flexibel wordt en makkelijk uitbreidbaar is. 

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.3]{../images/link.png}
	\caption{Link entiteit die 2 hub entiteiten met elkaar verbindt. \autocite{Linstedt2016}.}
	\label{fig:link}
\end{figure}

Ook bij links moeten er een aantal attributen aanwezig zijn:

\begin{itemize}
	\item \textbf{Hashkey (PK):} Als primaire sleutel van de entiteit worden alle business keys gehasht naar 1 sleutel.
	\item \textbf{LoadDate:} Datum/tijdstip wanneer de record geëxtraheerd werd uit de bron.
	\item \textbf{Record source:} Van welke databron is de record afkomstig?
	\item \textbf{Business key(s):} Alle business key(s) van de 2 gelinkte hub-entiteiten.
\end{itemize} 

\subsubsection{Sattelites}
In een satellite worden alle gegevens gestockeerd die een business object, relatie of transactie beschrijven. In de entiteit zelf is het belangrijk dat de historiek wordt bijgehouden \autocite{Linstedt2016}.

Bij een satellite vinden we minstens volgende attributen terug:

\begin{itemize}
	\item \textbf{Parent Hashkey (PK):} Als primaire sleutel van de entiteit worden alle business keys gehasht naar 1 sleutel samen met de naam van afkomsts.
	\item \textbf{LoadDate (PK):} Datum/tijdstip wanneer de record geëxtraheerd werd uit de bron.
	\item \textbf{Record source:} Van welke databron is de record afkomstig?
	\item \textbf{End load date:} Hierin wordt het moment geladen wanneer de entiteit niet meer gebruikt wordt (belangrijk voor het bewaren van de historiek). Wie vertrouwt is met het dimensioneel modelleren, kan dit vergelijken met het principe van slow changing dimensions.
	\item \textbf{Hash difference:} Hierin worden alle dimensionele data samen gehasht naar 1 sleutel. Zo kan er makkelijk vergeleken worden of de nieuwe data wel degelijk een verandering is of niet.
\end{itemize} 

Een satellite hoort bij een hub of een link. Een hub en een satellite vormen een bepaald business object. Hier is de primary key een composite key, dat bestaat uit een Parent Hashkey en een LoadDate. Deze combinatie is nodig om nieuwe versies van data te kunnen toevoegen.

\subsubsection{Data Vault schema}
Wanneer we zowel hubs, links als sattelites samengieten in één schema, bekomen we een data vault. Wel zijn er nog enkele belangrijke opmerkingen:

\begin{itemize}
	\item Hubs mogen nooit rechtstreeks verbonden met elkaar, dit moet altijd gebeuren via een link (anders verliest het model zijn flexibiliteit).
	\item Een link kan meer dan 2 hubs verbinden.
	\item Sattelites kunnen zowel met hubs als links verbonden worden.
	\item Hubs/links kunnen meerdere sattelites hebben: deze staan meestal voor verschillende databronnen.
	\item Een satellite kan maar verbonden worden met 1 hub of link. 
\end{itemize} 

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.7]{../images/dvmodel.png}
	\caption{Een voorbeeld van een data vault model (Bukhantsov.org)}
	\label{fig:dvmod}
\end{figure}

Dit model heeft veel flexibiliteit te bieden. Enerzijds kan er gemakkelijk nieuwe hubs toevoegen door een nieuwe link te leggen. Ook kan heel gemakkelijk een nieuwe gegevensbron toevoegen, dit wordt gedaan door een nieuwe satellite toe te voegen aan een bestaande hub. 

\section{Rapporteringsomgevingen}
\label{sec:omgeving}
Wanneer alle data in de data warehouse ingeladen en getransformeerd is, moet het mogelijk zijn om visuele en interactieve rapporten op te stellen. Op basis van deze rapporten kan het management beslissingen gaan nemen. Ook kunnen dashboards opgesteld worden voor werknemers die een high level overzicht nodig hebben. De bedoeling in deze sectie is om een schets te geven van de rapporteringsomgevingen die op de markt beschikbaar zijn. 

\subsection{Wat is een KPI?}
Een Key Performance Indicator (KPI) is een doelstelling uitgedrukt als een cijfer voor een bepaalde actie van een bedrijf. Een KPI is niet sector gebonden, maar wordt bepaald door het management van een onderneming. Het is een doelstelling die het bedrijf moet verwezelijken. Vaak worden deze doelstellingen op lange termijn bepaald. 

Een KPI moet opgesteld zijn aan de hand van het SMART-principe:
\begin{itemize}
	\item \textbf{Specifiek: } een KPI moet duidelijk geformuleerd worden.
	\item \textbf{Meetbaar: } Er moet kunnen vastgesteld worden wanneer een doel bereikt is.
	\item \textbf{Acceptabel:} Is de KPI acceptabel?
	\item \textbf{Realistisch: } Is het behalen van deze KPI realistisch?
	\item \textbf{Tijdsgebonden: } Wanneer moet de KPI behaald worden?
\end{itemize} 

Alle KPI's samen vormen de rapporteringsnoden op strategisch niveau.

\subsection{Het magische kwadrant}
\textcite{Gartner2019} maakte een vergelijkende studie tussen verschillende Business Intelligence platvormen. Als meetpunt werd in deze studie rekening gehouden met 15 cruciale features:
\begin{itemize}
	\item \textbf{Administratie, beveiliging en architectuur:} administratie gebruikers, beveiliging garanderen, controleren wie toegang heeft en indien nodig, een herstel kunnen uitvoeren.
	\item \textbf{Platform-as-a-service:} heeft de software een omgeving beschikbaar in de cloud?
	\item \textbf{Connecteren naar databronnen:} kunnen gebruikers verbinden naar verschillende databronnen (ongestructureerde data, gestructureerde data, ..)?
	\item \textbf{Beheer van metadata:} mogelijkheid aanbieden om te kunnen werken met metadata (zoeken, opslaan, herstellen, ...).
	\item \textbf{Opslag en laden:} mogelijkheden voor het integreren, transformeren en het laden van gegevens.
	\item \textbf{Voorbereiden van gegevens:}  op welke manieren is het voorbereiden van gegevens mogelijk? Kan er gebruikt worden van machine learning hierbij?
	\item \textbf{Schaalbaarheid:} hoe wordt er omgegaan met enorme volume data, complexe data modellen, zijn de prestaties geoptimaliseerd, en hoe gaat de omgeving om met veel gebruikers?
	\item \textbf{Geavanceerde analyses voor data scientists:} Zijn de geavanceerde analytische mogelijkheden makkelijk beschikbaar of moet er externe ontwikkelde software geïmplementeerd worden?
	\item \textbf{Dashboards:} De mogelijkheid om interactieve dashboards te creëren, is werken met geo-informatie mogelijk?
	\item \textbf{Verkennen:} kunnen gebruikers data analyseren en manipuleren door te werken met een interactieve presentatie van die gegevens?
	\item \textbf{Augmented data ontdekken:} kan data ontdekt worden door natural language query (NLQ) technologie? Kan het automatisch uitzonderingen, clusters, links en voorspellingen vinden en visualiseren?
	\item \textbf{Invoeren van Analytics in externe applicaties:} kunnen deze dashboards of visualiseringen in een andere omgeving makkelijk worden ingevoerd?
	\item \textbf{Publiceren, delen en samenwerken: } kunnen gebruikers de inhoud publiceren, delen of samenwerken met anderen om visualisaties te realiseren of bekijken?
	\item \textbf{Gebruiksgemak, visuele aantrekkelijkheid en de integratie van de workflow:} is het platvorm makkelijk te gebruiken en beheren? Kan de data visueel aantrekkelijk voorgesteld worden? Kan de tool makkelijk geïmplementeerd worden in huidige workflows?
\end{itemize} 

Op basis van deze features werden de verschillende Business Intelligence platvormen verdeeld in vier kwadranten naargelang hun visie en naar hoelang ze deze realiseren. De vier kwadranten zijn: niche-spelers, uitdagers, visionairs en marktleiders.

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.7]{../images/gartner.png}
	\caption{Het magische kwadrant opgesteld door \textcite{Gartner2019}.}
	\label{fig:dvmod}
\end{figure}

In deze literatuurstudie bekijken we enkel de mogelijkheden die 2 grootmachten in de ERP-markt (Microsoft en SAP) aanbieden, maar er zijn dus nog een aantal andere opties beschikbaar om rapporteringen visueel aantrekkelijk te maken. 

\subsection{SAP Analytics Cloud}
Bij SAP Analytics Cloud kan er gebruik gemaakt worden van realtime analytics. Hiervoor is live data nodig en deze wordt opgehaald in het transactionele systeem. Indien gewenst, kan er toch nog steeds gewerkt worden met een batch die 's nachts geladen wordt. 

Analytics cloud biedt heel veel mogelijkheden om interactieve dashboards te ontwerpen. Waar SAP Analytics Cloud zich onderscheidt met de andere spelers, is dat er in deze omgeving planning kan worden toegepast. Er kunnen budgetten opgesteld worden voor de komende jaren (bijvoorbeeld IT-kosten), en deze kunnen dan vergeleken worden met de actuele kosten op dat momenten. Wanneer een bedrijf deze planning-feature wil implementeren, moeten ze hiervoor een aparte licentie aanschaffen.

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.45]{../images/sac.png}
	\caption{Een voorbeeld van een dashboard gemaakt met SAP Analytics Cloud (sap.com)}
	\label{fig:sac}
\end{figure}

\subsection{Power BI}
Power BI is het programma bij uitstek dat gebruikt wordt bij datavisualisatie van ontwikkelaar Microsoft. PowerBI is een uitstekende keuze wanneer bedrijven Office 365 en microsoft Dynamics geïmplementeerd hebben in hun infrastructuur.

In tegenstelling tot SAP Analytics cloud kan je Power BI wel on-premise installeren. Maar indien gewenst, kan er steeds gebruik gemaakt worden van een cloud-omgeving.

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.45]{../images/powerbi.png}
	\caption{Een voorbeeld van een dashboard gemaakt met Power BI (microsoft.com)}
	\label{fig:powerbi}
\end{figure}
 





